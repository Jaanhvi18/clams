#######################################################
# Specify all the model hyperparameters here,
# then run the training/testing scripts.
#######################################################

# Main working directory
workdir=/home-1/amuelle8@jhu.edu/scratch/LM_syneval

# Directory where the Pytorch word language model code lives (with modifications for the multitask LMs)
lm_dir=$workdir/word-language-model

# Paths to LM & CCG data
lm_data_dir=/home-1/amuelle8@jhu.edu/scratch/fr_wiki/lm_dataset
ccg_data_dir=$workdir/data/ccg_data
train=train.txt
valid=valid.txt
test=test.txt

# Path to save the model
model_dir=$workdir/models/fr1

# TRAINING: Default LM hyperparameters
epochs=40
model=LSTM
log_freq=1000 # how often to display training progress
batch_size=20 # batch size
num_hid=800 # number of hidden units
lr=20.0 # learning rate
seed=$SLURM_JOBID # random seed
nlayers=2 # number of layers in network
order=5 # default order for an ngram model
dropout=0.2
